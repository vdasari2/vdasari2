<b>  ðŸ’¼ Professional Summary: </b> 
<br> Data Scientist and Analyst with expertise in SQL, Python, and advanced machine learning algorithms, focused on credit risk analytics, data engineering, and business intelligence. Proven ability to optimize risk strategies, build predictive models, and enhance data infrastructure using cloud platforms such as AWS and GCP. Skilled in developing and automating data pipelines, creating interactive dashboards, and deploying end-to-end ML solutions. Adept at leveraging statistical techniques and ML models to generate actionable insights, drive strategic decision-making, and mitigate operational risks.

<b>  ðŸ’¼ Portfolio Website: </b> 
https://6718760913824a2892d7e4af--glistening-duckanoo-eb3e64.netlify.app/

<b> ðŸ›  Technical Skills: </b>
<br> <b> Languages and Python Libraries: </b>
SQL, Python, NumPy, Pandas, Matplotlib, Beautiful Soup, Selenium

<b> Databases, Statistical, and Visualization Tools: </b> 
MS SQL, MongoDB, Snowflake DWH, SSIS, Matillion ETL, Power BI, Tableau, Apache Airflow

<b> Cloud Technologies: </b> 
AWS (S3, EC2, RDS, Redshift, etc.), GCP (BigQuery)

<b> Other Tools: </b> 
Knowledge Studio, DOMO, Grafana, JIRA, Confluence, Salesforce Sales Cloud, GIT, Postman

<b> ðŸ’¼ Professional Experience </b> 
<br> <b> Data Analyst, Product and Credit Risk Analytics </b> 
<br> <b> Lending Point, United States </b>
<br> <b> Jun 2023 â€“ Present </b> 

<br> * Performed comprehensive credit risk assessment on a $100M loan portfolio using Python, Pandas, NumPy, BigQuery SQL, and Tableau.
<br> * Employed Decision Trees, Random Forests, XGBoost with data from Bureau, Clarity, and LexisNexis to identify key risk indicators, resulting in a 17% reduction in potential losses through targeted risk mitigation strategies.
<br> * Leveraged CK LB MBE (Credit Karma Model Building Environment) and Jupyter Notebooks for data preprocessing and analysis of Equifax, TU, and Experian datasets, identifying credit attributes contributing to higher delinquency rates.
<br> * Utilized Chi-square tests, ANOVA, and Recursive Feature Elimination (RFE) to validate the statistical significance of variables, ensuring model reliability and optimizing feature selection.
<br> * Implemented K-means and DBSCAN clustering techniques to segment customers based on various credit bureau attributes, refining loan categorization and improving risk differentiation.
<br> * Developed data-driven risk strategies and revamped grading mechanisms using swap sets, cohort analysis, and time series analysis. Reduced overall risk exposure by 12% by optimizing key financial metrics such as ALR, RAY, and Delinquency %.
<br> * Utilized GCP BigQuery for client reporting file extraction and automated daily/monthly workflows using Power Automate and Apache Airflow, reducing manual reporting time by 70%.
<br> * Constructed and managed comprehensive DOMO and Tableau dashboards, leading to a 12% improvement in identifying at-risk loans at various delinquency stages (FPM 1/15 and 30/60/90/120 days past due).

<b> Data Engineer </b> 
<br> <b> National Grid, United States </b> 
<br> <b> Jan 2023 â€“ Jun 2023 </b> 

<br> * Part of modernizing National Gridâ€™s DWH platform by migrating legacy Oracle DWH to Snowflake Cloud Datawarehouse using SQL and sunset MicroStrategy by migrating reports to Power BI project.
<br> * Migration of 47 Downstate DataMartâ€™s â€“ Active (14), Inactive (33) â€“ Lift and shift of 33 inactive DataMartâ€™s from 8370 tables into Snowflake staging layer.
<br> * Migrated all 688 active data ingestion flows from SSIS to Matillion ETL using Python and SQL. Additionally, leveraged the DBT (Data Build Tool) framework to manage and orchestrate transformation workflows in Snowflake, ensuring clean and reliable data for analytics purposes.
<br> * Collaborated with Project Managers and Developers to design ETL solutions, leading to a 50% increase in report generation speed and a 25% improvement in data quality for analytics deliverables.
