ðŸ’¼ Professional Summary: 
Experienced Data Analyst and Engineer skilled in SQL, Python, Pandas, Tableau, ML Algorithms, AWS, and GCP. At Lending Point, optimized credit risk and growth strategies, actively contributing to Business Intelligence (BI), Product Management, and Partner Engagement initiatives. At National Grid, modernized data infrastructure with Snowflake Data Warehouse and data ingestion flows from SSIS using Matillion ETL. Proficient in visualization and tools like Knowledge Studio, DOMO, and Salesforce for generating actionable insights.

ðŸ›  Technical Skills
Languages and Python Libraries:
SQL, Python, NumPy, Pandas, Matplotlib, Beautiful Soup, Selenium

Databases, Statistical, and Visualization Tools:
MS SQL, MongoDB, Snowflake DWH, SSIS, Matillion ETL, Power BI, Tableau, Apache Airflow

Cloud Technologies:
AWS (S3, EC2, RDS, Redshift, etc.), GCP (BigQuery)

Other Tools:
Knowledge Studio, DOMO, Grafana, JIRA, Confluence, Salesforce Sales Cloud, GIT, Postman

ðŸ’¼ Professional Experience
Lending Point, United States
Data Analyst, Product and Credit Risk Analytics
Jun 2023 â€“ Present

Performed comprehensive credit risk assessment on a $100M loan portfolio using Python, Pandas, NumPy, BigQuery SQL, and Tableau.
Employed Decision Trees, Random Forests, XGBoost with data from Bureau, Clarity, and LexisNexis to identify key risk indicators, resulting in a 17% reduction in potential losses through targeted risk mitigation strategies.
Leveraged CK LB MBE (Credit Karma Model Building Environment) and Jupyter Notebooks for data preprocessing and analysis of Equifax, TU, and Experian datasets, identifying credit attributes contributing to higher delinquency rates.
Utilized Chi-square tests, ANOVA, and Recursive Feature Elimination (RFE) to validate the statistical significance of variables, ensuring model reliability and optimizing feature selection.
Implemented K-means and DBSCAN clustering techniques to segment customers based on various credit bureau attributes, refining loan categorization and improving risk differentiation.
Developed data-driven risk strategies and revamped grading mechanisms using swap sets, cohort analysis, and time series analysis. Reduced overall risk exposure by 12% by optimizing key financial metrics such as ALR, RAY, and Delinquency %.
Utilized GCP BigQuery for client reporting file extraction and automated daily/monthly workflows using Power Automate and Apache Airflow, reducing manual reporting time by 70%.
Constructed and managed comprehensive DOMO and Tableau dashboards, leading to a 12% improvement in identifying at-risk loans at various delinquency stages (FPM 1/15 and 30/60/90/120 days past due).
National Grid, United States
Data Engineer
Jan 2023 â€“ Jun 2023

Led the modernization of National Gridâ€™s DWH platform by migrating legacy Oracle DWH to Snowflake Cloud Data Warehouse using SQL.
Migrated 47 Downstate DataMarts â€“ Active (14) and Inactive (33) â€“ performing lift-and-shift operations for 8370 tables into the Snowflake staging layer.
Migrated 688 active data ingestion flows from SSIS to Matillion ETL using Python and SQL.
Leveraged the DBT (Data Build Tool) framework to manage transformation workflows in Snowflake, ensuring reliable and clean data for analytics.
Collaborated with Project Managers and Developers to design ETL solutions, improving report generation speed by 50% and enhancing data quality by 25%.
Supported the sunset of MicroStrategy by migrating reports to Power BI.
